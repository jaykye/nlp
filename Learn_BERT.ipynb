{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>BERT</h1>\n",
    "\n",
    "<li>BERT는 transformer encoder 아키텍쳐중 하나로, NLP에서 아주 성공적으로 사용되고 있다.<br>\n",
    "<li>이 아키텍쳐는 자연어를 vector space에 올려놓고 deep learning을 적용한다.<br>\n",
    "<li>BERT failmily의 모들들은 transformer 인커더 아키텍쳐를 사용해서 full context를 유지 한 채로, 각 text 인풋 토큰을 양방향으로 processing한다.<br>\n",
    "<li>BERT 모델들은 주로 미리 커다란 corpus of text를 사용해서 <b>미리 train 시켜놓고</b> 각 모델의 <b>사용용도에 맞춰 fine-tune해서 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # this is for AdamW optimizer -- 스페셜한 optimizer인가보네?\n",
    "import matplotlib.pyplot as plt\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 이 노트북에서는 Sentiment analysis를 시도해 본다.<br>\n",
    " Movie review 받아서 Positive인지 Negative인지 알아본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 39s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# IMDB 데이터를 다운받자.\n",
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 text_dataset_from_directory를 사용해서 tf.data.Dataset obj를 만들것이다.<br>\n",
    "또, 데이터는 train / test만 있고 validation set가 없으므로 train세트를 8:2로 나눠서 validation set을 만들자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "# batch 사이즈나 shuffle을 dataset단계에서 해결한다.\n",
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE) # Dataset는 모든 데이터를 한번에 불러오지 않는다.\n",
    "# 한 batch씩 읽고 train 할 동안에 다음 batch를 미리 load 시켜놓자.\n",
    "\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds = tf.keras.preprocessing.text_dataset_from_directory('aclImdb/test', batch_size=batch_size)  # tets세트는 섞을 이유가 없다.\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review: b'This cowardly and offensive film had me intrigued to begin with. The characters are the familiar dispossessed young males frequently to be seen hanging around bored in a sea side town. Robert is an outsider but he has his music which could have been his soul. Instead Clay makes Robert into a freak who embarks on a journey into cannabis and ecstasy and getting in with the wrong crowd. Clay seems to believe in \"reefer madness\" and Robert ends the film as a homicidal rapist. One wonders how much experience of real life this young director has. No one can save poor Robert. Clay leaves us with the message that young British men are out of control. A very unsubtle link is made to the Iraqi insurgents; during the needlessly graphic rape we are subjected to explosions and images of war. The film shows male peer group extremism pushed to it\\'s limits. The young bombers in London draw a parallel with Clay\\'s hateful depiction of modern male. Clay implies that men simply cannot help themselves from inflicting terrible acts of violence. It is a wonder the British film industry allows money to be invested in films which advocate such divisive propaganda, when in London we are still reeling from the recent attacks. This is Clay\\'s first film, I would be delighted if it is his last.'\n",
      "Label : 0 (neg)\n",
      "review: b'The Last Hunt is the forgotten Hollywood classic western. The theme of genocide via buffalo slaughter is present in other films but never so savagely. Robert Taylor\\'s against-type role as the possessed buffalo and Indian killer is his finest performance.<br /><br />In the 1950s, your mom dropped you and your friends off at the Saterday matin\\xc3\\xa9e, usually featuring a western or comedy. But it was wrong then and now to let a youngster watch psycho-dramas like The Searchers and The Last Hunt. Let the kids wait a few years before exposing them to films with repressed sexual sadism and intense racial hatred.<br /><br />Why did Mom fail to censor these films? Because they featured \"safe\" Hollywood stars like Taylor and John Wayne. But the climatic scene in The Last Hunt is as horrifying as Vincent Price\\'s mutation in The Fly.<br /><br />The mythology of the white buffalo, part of the texture of this movie, was later ripped-off by other movies including The White Buffalo, starring Charles Bronson as Wild Bill Hickock. The laugh here is that Bronson used to play Indians.<br /><br />Today a large remnant bison herd resides in Yellowstone National Park in Wyoming. In the winter, hunger forces surplus animals out of the park into Montana, where they are sometimes harvested by Idaho\\'s Nez Perce Indians under a US treaty right that pre-dates the Lincoln Presidency. Linclon signed the Congressional act which authorized the continental railroad and started the buffalo slaughter.'\n",
      "Label : 1 (pos)\n",
      "review: b'How did this become a blockbuster? Dear God I don\\'t know where to start why this movie sucked too much. The movie was predictable & there was no originality. The only thing I can admire is the acting of some characters. The movie was too bright, they should have done something with the lighting, eg. making the environment more darker. The make up on certain dead characters made this movie look like a 1970 horror flick. This is 2006! People don\\'t get scared by other people wearing heavy make up. Most of the horror scenes we\\'re taken from other Hollywood or Asian horror movies. Total rip off! This is why I don\\'t watch tagalog movies. The only reason why so many people \"screamed\" while watching this movie is because of conformity. How many times do we have to copy scenes from The Ring and improvise it that instead of the girl coming out of the TV, its now coming from the window next door? No matter how you put it, ITS STILL A RIP OFF. If you want a good horror movie, go watch the 50 best horror movie listed on this website.'\n",
      "Label : 0 (neg)\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(3):\n",
    "        print(f\"review: {text_batch.numpy()[i]}\")\n",
    "        label=label_batch.numpy()[i]\n",
    "        print(f'Label : {label} ({class_names[label]})')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    print(label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아까 말했던 대로, BERT 모델은 미리 train되어 있는 것을 불러와서 내 목적에 맞게 fine tune해서 쓸꺼야.<br>\n",
    "Google에서 만든 여러 모델이 있는데, \n",
    "모델 불러오자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
