{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow found 1 GPU.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "tfds.disable_progress_bar()\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(f'Tensorflow found {len(physical_devices)} GPU.')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Dataset에 대해서 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 Dataset는 한번에 모든 데이터를 메모리에 올리기 힘드니깐 하나씩 스트리밍시키는 방법임. 아마 generater 같은 개념일듯.\n",
    "여기서는 tensorflow에서 기본적으로 제공하는 데이터세트를 받아서 사용해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>첫째로 MNIST 데이터세트를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load('mnist', split=['train', 'test'], shuffle_files=True, as_supervised=True, with_info=True)\n",
    "# As tuple (as_supervised=True)\n",
    "# By using as_supervised=True, you can get a tuple (features, label) instead for supervised datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "    # Normalize images\n",
    "    return tf.cast(image, tf.float32)/255.0, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 64\n",
    "ds_train = ds_train.map(normalize_img, num_parallel_calls=AUTOTUNE)  # normalization 프로세스\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)  # 이게 오히려 맞는 방법인듯.\n",
    "ds_train = ds_train.batch(BATCH_SIZE)\n",
    "ds_train = ds_train.prefetch(AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.map(normalize_img, num_parallel_calls=AUTOTUNE)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 - 15s - loss: 0.2142 - accuracy: 0.9396\n",
      "Epoch 2/5\n",
      "938/938 - 2s - loss: 0.0763 - accuracy: 0.9777\n",
      "Epoch 3/5\n",
      "938/938 - 2s - loss: 0.0543 - accuracy: 0.9837\n",
      "Epoch 4/5\n",
      "938/938 - 2s - loss: 0.0430 - accuracy: 0.9870\n",
      "Epoch 5/5\n",
      "938/938 - 2s - loss: 0.0346 - accuracy: 0.9890\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 0.0697 - accuracy: 0.9786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0697445347905159, 0.978600025177002]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(ds_info.features['image'].shape),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.Adam(lr=0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(ds_train, epochs=5, verbose=2)\n",
    "model.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 코드를 사용해서 데이터세트 예시를 볼 수 있다. -- 이미지 파일만 가능한듯.\n",
    "tfds.show_examples(mnist_dataset['train'], mnist_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>여기부터는 IMDB 데이터세트를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "(imdbds_train, immdbds_test), imdb_info = tfds.load(\n",
    "    'imdb_reviews',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    full_name='mnist/3.0.1',\n",
       "    description=\"\"\"\n",
       "    The MNIST database of handwritten digits.\n",
       "    \"\"\",\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    data_path='C:\\\\Users\\\\kye09\\\\tensorflow_datasets\\\\mnist\\\\3.0.1',\n",
       "    download_size=11.06 MiB,\n",
       "    dataset_size=21.00 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP를 하려면 1. Vocab 만들기 2. Tokenizing(encoding) 3. embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.features.text.Tokenizer()\n",
    "\n",
    "# 1. vocab 만들기\n",
    "def build_vocabulary():\n",
    "    '''데이터 세트를 한번 훑으면서 vocab 만들기. 그래도 하나씩만 꺼내오니깐 메모리 덜먹음.'''\n",
    "    vocabulary = set()\n",
    "    for text, _ in ds_train:\n",
    "        vocabulary.update(tokenizer.tokenize(text.numpy().lower()))\n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = build_vocabulary()\n",
    "\n",
    "encoder = tfds.featuers.text.TokenTextencoder(\n",
    "    vocabulary, oov_token = '<UNK>', lowercase=True, tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "def my_encoding(text_tensor, label):\n",
    "    '''use tensoflow's encoder to process words in text to indices of vocab'''\n",
    "    return encoder.encode(text_tensor.numpy()), label\n",
    "\n",
    "# 여기서 my_encoding 함수를 tensorflow graph에 포함시키고 싶다. 왜냐하면 dataset이 하나씩 stream해 주는 것을\n",
    "# 받아서 process 해야 하는 과정이기 때문. 데이터 전부가 메모리에 올라와 있었다면 이런거 안해도 되지. \n",
    "# 다른 방법으로는 그냥 tensorflow에서 주는 encoding, embedding layer 사용해면 된다. 만약 layer로 제공되지 않는 process를 해야한다면 \n",
    "# 이렇게 해라.\n",
    "\n",
    "def encode_map(text, label): \n",
    "    encoded_text, label = tf.py_function(\n",
    "        my_encoding, inp=[text, label], Tout=(tf.int64, tf.int64)\n",
    "    )\n",
    "    encoded_text.set_shape([None])\n",
    "    label.set_shape([])  # integer\n",
    "    return encoded_text, label \n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "ds_train = ds_train.map(encoded_map, num_parallel_calls=AUTOTUNE).cache()\n",
    "ds_train = ds_train.shuffle(10_000)\n",
    "ds_train = ds_train.padded_batch(32, padded_shapes=([None], ()))  # specify which axis to pad.(None part)\n",
    "ds_train = ds_train.prefetch(AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.map(encode_map)\n",
    "ds_test = ds_test.padded_batch(32, padded_shape=([None], ()))\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Masking(mask_value=0) # 패딩 파트에 0을 집어넣고 거기는 computation하지 말라고 말해주는 것.\n",
    "    tf.keras.layers.Embedding(input_dim=len(vocabulary)+2, output_dim=32)  # 왜 +2나고? 1. 패딩에 들어가는 0, 2. OOV(<UNK>) output_dim이 embedding dimension임.\n",
    "    # 보통 한 300정도 쓴다. 여튼 각 단어마다 32개의 dimension을 부여함.\n",
    "    tf.keras.layers.GlobalAveragePool1D(), # 이전 레이어에서 나온 32 dim마다 average를 계산해줌\n",
    "    '''GAP layers perform a more extreme type of dimensionality reduction, \n",
    "    where a tensor with dimensions h×w×d is reduced in size to have dimensions 1×1×d. \n",
    "    GAP layers reduce each h×w feature map to a single number by simply taking the average of all hw values.'''\n",
    "    tf.keras.layers.Dense(64, activation = 'relu')\n",
    "    tf.keras.layers.Dense(1)  # >0 --> positive, <0 --> negative review\n",
    "])\n",
    "\n",
    "# 아까는 class가 2개 초과 였으므로 sparse categorical cross entropy사용\n",
    "# 2개일 때는 binary\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(3e-4, clipnorm=1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(ds_train, epoch=10, verbose=2)\n",
    "model.evaluate(ds_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
